{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluate RAG with LlamaIndex"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SxQ2qzb7DPu1"
      },
      "source": [
        "In this notebook we will look into building an RAG pipeline and evaluating it with LlamaIndex. It has following 3 sections.\n",
        "\n",
        "1. Understanding Retrieval Augmented Generation (RAG).\n",
        "2. Building RAG with LlamaIndex.\n",
        "3. Evaluating RAG with LlamaIndex."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jYKmLpi0-AvJ"
      },
      "source": [
        "**Retrieval Augmented Generation (RAG)**\n",
        "\n",
        "LLMs are trained on vast datasets, but these will not include your specific data. Retrieval-Augmented Generation (RAG) addresses this by dynamically incorporating your data during the generation process. This is done not by altering the training data of LLMs, but by allowing the model to access and utilize your data in real-time to provide more tailored and contextually relevant responses.\n",
        "\n",
        "In RAG, your data is loaded and prepared for queries or “indexed”. User queries act on the index, which filters your data down to the most relevant context. This context and your query then go to the LLM along with a prompt, and the LLM provides a response.\n",
        "\n",
        "Even if what you’re building is a chatbot or an agent, you’ll want to know RAG techniques for getting data into your application."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![RAG Overview](../../images/llamaindex_rag_overview.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv9e2I1w-SLF"
      },
      "source": [
        "**Stages within RAG**\n",
        "\n",
        "There are five key stages within RAG, which in turn will be a part of any larger application you build. These are:\n",
        "\n",
        "**Loading:** this refers to getting your data from where it lives – whether it’s text files, PDFs, another website, a database, or an API – into your pipeline. LlamaHub provides hundreds of connectors to choose from.\n",
        "\n",
        "**Indexing:** this means creating a data structure that allows for querying the data. For LLMs this nearly always means creating vector embeddings, numerical representations of the meaning of your data, as well as numerous other metadata strategies to make it easy to accurately find contextually relevant data.\n",
        "\n",
        "**Storing:** Once your data is indexed, you will want to store your index, along with any other metadata, to avoid the need to re-index it.\n",
        "\n",
        "**Querying:** for any given indexing strategy there are many ways you can utilize LLMs and LlamaIndex data structures to query, including sub-queries, multi-step queries and hybrid strategies.\n",
        "\n",
        "**Evaluation:** a critical step in any pipeline is checking how effective it is relative to other strategies, or when you make changes. Evaluation provides objective measures of how accurate, faithful and fast your responses to queries are."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Besynnjg_Cg9"
      },
      "source": [
        "## Build RAG system.\n",
        "\n",
        "Now that we have understood the significance of RAG system, let's build a simple RAG pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "t1NdWoBI_OFR"
      },
      "outputs": [],
      "source": [
        "# The nest_asyncio module enables the nesting of asynchronous functions within an already running async loop.\n",
        "# This is necessary because Jupyter notebooks inherently operate in an asynchronous loop.\n",
        "# By applying nest_asyncio, we can run additional async functions within this existing loop without conflicts.\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from llama_index.core.llama_dataset.generator import RagDatasetGenerator\n",
        "from llama_index.core.indices import VectorStoreIndex\n",
        "from llama_index.core.readers import SimpleDirectoryReader\n",
        "from llama_index.core.service_context import ServiceContext\n",
        "from llama_index.core.node_parser import SimpleNodeParser\n",
        "from llama_index.core.evaluation import (\n",
        "    generate_question_context_pairs,\n",
        ")\n",
        "from llama_index.core.evaluation.retrieval.evaluator import (\n",
        "    RetrieverEvaluator,\n",
        ")\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "import os\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GocbeIV3rQYc"
      },
      "source": [
        "Set Your OpenAI API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bocDlS3FrP8L"
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY'] = 'YOUR OPENAI API KEY'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oKXIZyFOKIIT"
      },
      "source": [
        "Let's use [Paul Graham Essay text](https://www.paulgraham.com/worked.html) for building RAG pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjyT5I-kKPbl"
      },
      "source": [
        "#### Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUOKqSSeCkEN",
        "outputId": "17c6b9f6-f6f6-4d9f-d75f-a197133f15f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 75042  100 75042    0     0   285k      0 --:--:-- --:--:-- --:--:--  284k\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p 'data/paul_graham/'\n",
        "!curl 'https://raw.githubusercontent.com/run-llama/llama_index/9b28893d0bd26014862d275d3b81a2918b3d05ff/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -o 'data/paul_graham/paul_graham_essay.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7_DxX6xKUbH"
      },
      "source": [
        "#### Load Data and Build Index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vCJln_YKaK-",
        "outputId": "0004d366-35a3-49d4-806c-86a307aaa560"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Doc ID: 30dda7f9-ea4a-4851-870e-949464d8565f\n",
            "Text: What I Worked On  February 2021  Before college the two main\n",
            "things I worked on, outside of school, were writing and programming. I\n",
            "didn't write essays. I wrote what beginning writers were supposed to\n",
            "write then, and probably still are: short stories. My stories were\n",
            "awful. They had hardly any plot, just characters with strong feelings,\n",
            "which I ...\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.settings import Settings\n",
        "\n",
        "documents = SimpleDirectoryReader(\"/home/aeva2/sci/openai-cookbook/examples/evaluation/data/paul_graham/\").load_data()\n",
        "# Define an LLM\n",
        "llm = OpenAI(model=\"gpt-4o-mini\")\n",
        "# Settings.llm = llm\n",
        "\n",
        "# # Build index with a chunk_size of 512\n",
        "node_parser = SimpleNodeParser.from_defaults(chunk_size=512)\n",
        "nodes = node_parser.get_nodes_from_documents(documents)\n",
        "vector_index = VectorStoreIndex(nodes)\n",
        "# vector_index = VectorStoreIndex.from_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65FyKpfY_inX"
      },
      "source": [
        "Build a QueryEngine and start querying."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kOZBy--R_m3I"
      },
      "outputs": [],
      "source": [
        "query_engine = vector_index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "G7NVP-N4_rXF"
      },
      "outputs": [],
      "source": [
        "response_vector = query_engine.query(\"What did the author do growing up?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPYx0ycS_x9B"
      },
      "source": [
        "Check response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "BFFLl1d7_4r4",
        "outputId": "74c2bc81-f326-4981-c7ac-92eb836e5e99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The author worked on writing short stories and programming, particularly on an IBM 1401 computer using an early version of Fortran in 9th grade.'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response_vector.response"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IfhTYwoO_8cd"
      },
      "source": [
        "By default it retrieves `two` similar nodes/ chunks. You can modify that in `vector_index.as_query_engine(similarity_top_k=k)`.\n",
        "\n",
        "Let's check the text in each of these retrieved nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcniGJVt_5V8",
        "outputId": "52b9d3da-30e7-460e-ab50-0ac3dc98ea1e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'What I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn\\'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\\n\\nThe language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.\\n\\nI was puzzled by the 1401. I couldn\\'t figure out what to do with it. And in retrospect there\\'s not much I could have done with it. The only form of input to programs was data stored on punched cards, and I didn\\'t have any data stored on punched cards. The only other option was to do things that didn\\'t rely on any input, like calculate approximations of pi, but I didn\\'t know enough math to do anything interesting of that type. So I\\'m not surprised I can\\'t remember any programs I wrote, because they can\\'t have done much. My clearest memory is of the moment I learned it was possible for programs not to terminate, when one of mine didn\\'t. On a machine without time-sharing, this was a social as well as a technical error, as the data center manager\\'s expression made clear.\\n\\nWith microcomputers, everything changed.'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# First retrieved node\n",
        "response_vector.source_nodes[0].get_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"I also worked on spam filters, and did some more painting. I used to have dinners for a group of friends every thursday night, which taught me how to cook for groups. And I bought another building in Cambridge, a former candy factory (and later, twas said, porn studio), to use as an office.\\n\\nOne night in October 2003 there was a big party at my house. It was a clever idea of my friend Maria Daniels, who was one of the thursday diners. Three separate hosts would all invite their friends to one party. So for every guest, two thirds of the other guests would be people they didn't know but would probably like. One of the guests was someone I didn't know but would turn out to like a lot: a woman called Jessica Livingston. A couple days later I asked her out.\\n\\nJessica was in charge of marketing at a Boston investment bank. This bank thought it understood startups, but over the next year, as she met friends of mine from the startup world, she was surprised how different reality was. And how colorful their stories were. So she decided to compile a book of interviews with startup founders.\\n\\nWhen the bank had financial problems and she had to fire half her staff, she started looking for a new job. In early 2005 she interviewed for a marketing job at a Boston VC firm. It took them weeks to make up their minds, and during this time I started telling her about all the things that needed to be fixed about venture capital. They should make a larger number of smaller investments instead of a handful of giant ones, they should be funding younger, more technical founders instead of MBAs, they should let the founders remain as CEO, and so on.\\n\\nOne of my tricks for writing essays had always been to give talks. The prospect of having to stand up in front of a group of people and tell them something that won't waste their time is a great spur to the imagination. When the Harvard Computer Society, the undergrad computer club, asked me to give a talk, I decided I would tell them how to start a startup. Maybe they'd be able to avoid the worst of the mistakes we'd made.\""
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Second retrieved node\n",
        "response_vector.source_nodes[1].get_text()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnvSrTeADNrX"
      },
      "source": [
        "We have built a RAG pipeline and now need to evaluate its performance. We can assess our RAG system/query engine using LlamaIndex's core evaluation modules. Let's examine how to leverage these tools to quantify the quality of our retrieval-augmented generation system."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jAMiOEsT_-o0"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Evaluation should serve as the primary metric for assessing your RAG application. It determines whether the pipeline will produce accurate responses based on the data sources and a range of queries.\n",
        "\n",
        "While it's beneficial to examine individual queries and responses at the start, this approach may become impractical as the volume of edge cases and failures increases. Instead, it may be more effective to establish a suite of summary metrics or automated evaluations. These tools can provide insights into overall system performance and indicate specific areas that may require closer scrutiny.\n",
        "\n",
        "In a RAG system, evaluation focuses on two critical aspects:\n",
        "\n",
        "*   **Retrieval Evaluation:** This assesses the accuracy and relevance of the information retrieved by the system.\n",
        "*   **Response Evaluation:** This measures the quality and appropriateness of the responses generated by the system based on the retrieved information."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "arGZqWQNIitt"
      },
      "source": [
        "#### Question-Context Pair Generation:\n",
        "\n",
        "For the evaluation of a RAG system, it's essential to have queries that can fetch the correct context and subsequently generate an appropriate response. `LlamaIndex` offers a `generate_question_context_pairs` module specifically for crafting questions and context pairs which can be used in the assessment of the RAG system of both Retrieval and Response Evaluation. For more details on Question Generation, please refer to the [documentation](https://docs.llamaindex.ai/en/stable/examples/evaluation/QuestionGeneration.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jK0BWZ88LjDq",
        "outputId": "87d11e23-50cf-4ff3-a449-12264cfd8ab8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 61/61 [01:24<00:00,  1.39s/it]\n"
          ]
        }
      ],
      "source": [
        "qa_dataset = generate_question_context_pairs(\n",
        "    nodes,\n",
        "    llm=llm,\n",
        "    num_questions_per_chunk=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjs2cgTpOfLM"
      },
      "source": [
        "#### Retrieval Evaluation:\n",
        "\n",
        "We are now prepared to conduct our retrieval evaluations. We will execute our `RetrieverEvaluator` using the evaluation dataset we have generated.\n",
        "\n",
        "We first create the `Retriever` and then define two functions: `get_eval_results`, which operates our retriever on the dataset, and `display_results`, which presents the outcomes of the evaluation."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bE1Z77YyPNwE"
      },
      "source": [
        "Let's create the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fV9IdnwLM_aw"
      },
      "outputs": [],
      "source": [
        "retriever = vector_index.as_retriever(similarity_top_k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLSNg2sSPc2U"
      },
      "source": [
        "Define `RetrieverEvaluator`. We use **Hit Rate** and **MRR** metrics to evaluate our Retriever.\n",
        "\n",
        "**Hit Rate:**\n",
        "\n",
        "Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it’s about how often our system gets it right within the top few guesses.\n",
        "\n",
        "**Mean Reciprocal Rank (MRR):**\n",
        "\n",
        "For each query, MRR evaluates the system’s accuracy by looking at the rank of the highest-placed relevant document. Specifically, it’s the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it’s second, the reciprocal rank is 1/2, and so on.\n",
        "\n",
        "Let's check these metrics to check the performance of out retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "H6V_LCxrPQzp"
      },
      "outputs": [],
      "source": [
        "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
        "    [\"mrr\", \"hit_rate\"], retriever=retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "NYFgmnpRPX-x"
      },
      "outputs": [],
      "source": [
        "# Evaluate\n",
        "eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3FLlvjoSbI5"
      },
      "source": [
        "Let's define a function to display the Retrieval evaluation results in table format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "S9T268MhRNxp"
      },
      "outputs": [],
      "source": [
        "def display_results(name, eval_results):\n",
        "    \"\"\"Display results from evaluate.\"\"\"\n",
        "\n",
        "    metric_dicts = []\n",
        "    for eval_result in eval_results:\n",
        "        metric_dict = eval_result.metric_vals_dict\n",
        "        metric_dicts.append(metric_dict)\n",
        "\n",
        "    full_df = pd.DataFrame(metric_dicts)\n",
        "\n",
        "    hit_rate = full_df[\"hit_rate\"].mean()\n",
        "    mrr = full_df[\"mrr\"].mean()\n",
        "\n",
        "    metric_df = pd.DataFrame(\n",
        "        {\"Retriever Name\": [name], \"Hit Rate\": [hit_rate], \"MRR\": [mrr]}\n",
        "    )\n",
        "\n",
        "    return metric_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "A1eESYN-RRgl",
        "outputId": "ff27adb0-d189-4b7d-8998-6df15b6a2014"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Retriever Name</th>\n",
              "      <th>Hit Rate</th>\n",
              "      <th>MRR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>OpenAI Embedding Retriever</td>\n",
              "      <td>0.729508</td>\n",
              "      <td>0.590164</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Retriever Name  Hit Rate       MRR\n",
              "0  OpenAI Embedding Retriever  0.729508  0.590164"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "display_results(\"OpenAI Embedding Retriever\", eval_results)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Observation:\n",
        "\n",
        "The Retriever with OpenAI Embedding  demonstrates a performance with a hit rate of `0.7586`, while the MRR, at `0.6206`, suggests there's room for improvement in ensuring the most relevant results appear at the top. The observation that MRR is less than the hit rate indicates that the top-ranking results aren't always the most relevant. Enhancing MRR could involve the use of rerankers, which refine the order of retrieved documents. For a deeper understanding of how rerankers can optimize retrieval metrics, refer to the detailed discussion in our [blog post](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPxAHE3kSjsT"
      },
      "source": [
        "#### Response Evaluation:\n",
        "\n",
        "1. FaithfulnessEvaluator: Measures if the response from a query engine matches any source nodes which is useful for measuring if the response is hallucinated.\n",
        "2. Relevancy Evaluator: Measures if the response + source nodes match the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "-zMMJAQvRS8H"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Describe the early programming experience of the author with the IBM 1401. What challenges did they face while trying to write programs, and how did the limitations of the technology impact their learning?', \"How did the author's approach to writing evolve before college, and what specific characteristics did their early short stories exhibit?\", \"Describe the differences in user experience between programming on the IBM 1401 and using a microcomputer like the TRS-80. How did these differences impact the author's approach to programming?\", 'What were some of the projects the author undertook after acquiring the TRS-80, and how did these projects reflect the capabilities of microcomputers compared to earlier computing systems?', \"Describe the author's initial experience with computers and programming. What specific projects did they undertake with their TRS-80, and how did these experiences shape their interest in technology?\", \"What factors influenced the author's decision to switch from studying philosophy to pursuing a career in artificial intelligence (AI)? Mention specific works or experiences that played a role in this transition.\", 'What inspired the author to switch their focus from philosophy to artificial intelligence, and which specific works influenced this decision?', \"Describe the author's experience with programming languages during their time at Cornell and how it impacted their understanding of AI development.\", \"What was the author's undergraduate thesis project, and why did they find it particularly exciting to work on it?\", \"How did the author's experience at Cornell influence their perception of Artificial Intelligence, and what realization did they come to during their first year of graduate school regarding the nature of AI?\", \"Based on the author's experience in graduate school, what realization did they come to regarding the limitations of traditional AI programs, and how did this influence their decision to focus on Lisp?\", \"Discuss the author's perspective on the relationship between theory and systems in computer science. What did they find more exciting, and what concern did they express about the longevity of systems work?\", \"Discuss the author's perspective on the relationship between theory and systems in computer science. How does the author differentiate between the two, and what personal inclination does he express regarding his work in the field?\", \"Reflecting on the author's experience at the Carnegie Institute, what realization did he come to about the nature of certain creations, and how did this influence his thoughts on the longevity and value of his work in computer science?\", 'What realization did the author have while visiting the Carnegie Institute that influenced their perspective on creating lasting work, and how did this realization contrast with their experiences in the field of software development?', \"Describe the author's journey in balancing their PhD program in computer science with their newfound interest in art. What challenges or conflicts might arise from pursuing multiple passions simultaneously?\", 'What internal conflict does the narrator experience regarding their academic pursuits and artistic aspirations while enrolled in a PhD program in computer science?', 'How did the narrator manage to meet the graduation deadline for their dissertation, and what topic did they choose to write about?', 'What topic did the author choose for their dissertation, and what was their initial reaction when asked about their progress towards graduation?', 'Describe the circumstances that led the author to apply to art schools, including the schools they applied to and the outcome of their applications.', 'What were the fundamental subjects included in the foundation classes that the author had to take at RISD, and why were these classes important for their artistic development?', 'Reflecting on the author\\'s experience at the Accademia, what does the phrase \"an arrangement whereby the students wouldn\\'t require the faculty to teach anything\" suggest about the dynamics of the educational environment there?', 'Discuss the dynamics between students and faculty at the Accademia as described in the passage. How does this relationship impact the learning environment for the students?', 'Compare and contrast the challenges of painting still lives versus painting live models, as outlined in the text. What advantages does the author find in painting still lives?', 'Compare and contrast the process of painting still lives with painting people as described in the text. What are the key differences in approach and technique?', 'According to the author, what is the significance of visual perception in everyday life, and how does painting still lives alter this perception?', \"Discuss the significance of low-level visual processes in everyday life as described in the text. How does the act of painting still lives challenge these processes and enhance one's observational skills?\", \"What was the author's experience at the Accademia, and what factors influenced their decision to return to the United States after the first year?\", \"What inspired Interleaf to add a scripting language to their software, and how did this decision impact the author's experience as an employee there?\", \"How did the author's financial situation change from their time in Florence to their job at Interleaf, and what were the implications of this change for their future plans?\", \"Based on the author's experiences at Interleaf, what key lesson did they learn about the relationship between company leadership and product development, and how did this influence their future ventures like Viaweb and Y Combinator?\", \"How did the author's financial situation change after securing a contract to publish On Lisp, and what strategies did they employ to manage their expenses during that time?\", 'What lesson did the author learn about the relationship between entry-level options and prestige in the context of business, and how did this influence their work at Viaweb and Y Combinator?', \"How does the author compare the experience of attending art school to that of medical school, particularly in relation to the painting department's focus on self-expression versus rigorous training?\", 'What is meant by a \"signature style\" in the context of painting, and how does it relate to the value of artwork in the art market?', \"Describe the author's experience at RISD and the reasons for their decision to drop out. What factors influenced their move to New York?\", 'What motivated the author to drop out of RISD in 1993, and how did their living situation change after leaving college?', \"Describe the significance of Idelle Weber in the author's artistic journey and the impact she had on her students.\", \"What motivated the author to write a book on Lisp, and how did they envision their lifestyle as a result of the book's success?\", \"Describe the author's experience with Idelle Weber and how it influenced their artistic pursuits. What significant realization did the author have while stretching a canvas in her studio?\", 'What realization did the author have about the potential of the World Wide Web after visiting Robert Morris at Harvard, and how did this influence their decision to start a company?', 'Describe the challenges the author and Robert faced when trying to sign up art galleries for their online services, and how did their focus shift when they observed the emergence of online stores?', \"What innovative idea did the author have while lying on a mattress in Robert's apartment that changed the approach to building online stores?\", 'Describe the initial challenges faced by the author and Robert when they decided to create software for online stores, particularly regarding the platform they intended to use.', 'What was the primary innovation that the founders of Viaweb aimed to achieve with their web app, and how did it differ from traditional software development practices at the time?', 'Describe the financial situation of the author before the seed funding was secured. How did this situation influence the urgency to launch their new company?', \"What percentage of the company was given to the individual who provided initial legal work and business advice, and how did this arrangement influence future startup funding models like Y Combinator's?\", \"Describe the main goal of the online store builder mentioned in the text and explain how the author's background in art contributed to achieving this goal.\", 'Describe the roles of the three main programmers mentioned in the text and identify which part of the software each was responsible for developing.', 'What challenges did the team face when launching their online stores in January 1996, and how did the timing of their launch impact their business?', 'What were the three main components of the software developed by the team, and who was responsible for each part?', \"How did the founders' financial situation influence the pricing strategy for their ecommerce software, and what unintended advantages did this create in the competitive landscape?\", 'What pricing strategy did the company initially adopt for small and big stores, and what was the rationale behind this decision?', \"How did the founders' approach to building stores for users contribute to their understanding of retail and the development of their software?\", 'What realization did the author come to regarding the importance of growth rate in a startup, and how did this differ from their initial perspective on user numbers?', \"How did the author's hiring decisions, influenced by investor expectations and industry norms during the Internet Bubble, impact the company's financial stability and eventual acquisition by Yahoo?\", \"What factors contributed to the decision to hire more employees at Viaweb, and how did this impact the company's financial trajectory before its acquisition by Yahoo?\", \"Describe the emotional and lifestyle changes experienced by the author after Yahoo's acquisition of Viaweb, particularly in relation to their work habits and personal possessions.\", \"How did the author's perception of their lifestyle change after Yahoo acquired Viaweb, and what specific purchase did they make that symbolized this change?\", \"What factors contributed to the author's decreased productivity during their time at Yahoo, and how did their experience at Yahoo compare to their previous work environment at Interleaf?\", \"What were the primary motivations for the narrator's decision to leave their job at Yahoo, and how did their boss perceive this decision?\", 'Describe the challenges the narrator faced when attempting to pursue painting after leaving their job. How did their living situation contribute to these challenges?', \"How did the narrator's experience of returning to New York differ from their time in California, particularly in terms of lifestyle and opportunities available to them?\", 'What innovative idea did the narrator have in the spring of 2000 regarding web applications, and how does it reflect their understanding of the future of technology?', 'Describe the innovative approach the author took in their painting process. How did this method reflect their experimentation with art during the spring of 2000?', \"What was the author's vision for a web app in 2000, and what challenges did they face in recruiting collaborators for this project?\", \"What were the initial motivations behind the author's decision to start a new company, and how did those motivations change over time?\", 'Describe the roles of the individuals recruited by the author for the new company, Aspra, and the specific tasks they were assigned during the development process.', 'What motivated the author to shift from building a company focused on a new architecture to developing an open source project, and what was the name of the new dialect of Lisp they created?', \"How did the author's experience at a Lisp conference and the subsequent online response to their talk influence their perception of the audience for their work?\", 'What realization did the author have after receiving 30,000 page views on their postscript file following a talk at a Lisp conference, and how did this realization change their perspective on publishing?', 'In what ways did the transition from print to online publishing impact the accessibility of essay writing for individuals outside of established publishing channels?', \"Discuss the author's perspective on the evolution of essay publishing from the print era to the online medium. What implications does this shift have for aspiring writers?\", \"Analyze the author's views on the relationship between prestige and the type of work one chooses to pursue. How does the author suggest that a lack of prestige can be indicative of genuine interest and motivation?\", \"Discuss the significance of pursuing work that lacks prestige, as mentioned in the context. How does this relate to the author's views on ambition and motives?\", \"Describe the event that took place in October 2003 at the author's house. What was the purpose of the party, and who was one of the notable guests that the author met there?\", \"What innovative idea did Maria Daniels propose for the party at the narrator's house, and how did it influence the narrator's personal life?\", 'According to the narrator, what are some suggested changes to the venture capital industry that he discussed with Jessica Livingston during her job search?', 'What were some of the key criticisms mentioned about the venture capital industry that the author discussed with Jessica during her job interview process?', \"How did the author's experience giving a talk at the Harvard Computer Society influence his decision to pursue angel investing?\", 'What motivated the author to start their own investment firm, and how did their previous experiences influence this decision?', 'In what ways did the author and their team plan to support startups beyond making seed investments, and what specific challenges did they aim to address for founders?', 'What were the primary differences between VC firms and angel investors as described in the context, and how did these differences impact the support available to founders?', 'How did the founders of YC come to adopt the batch model for funding startups, and what was the rationale behind organizing a summer program for undergraduates to start startups?', 'What was the primary motivation behind organizing the Summer Founders Program, and how did the founders perceive their roles as investors and the participants as founders?', 'How did the founders of the Summer Founders Program source applications, and what was the outcome in terms of the number of applications received and the selection process?', 'What was the primary motivation behind the creation of the Summer Founders Program, and how did it differ from traditional summer job opportunities for undergraduates?', 'Describe the funding structure of the Summer Founders Program and explain how it was perceived as advantageous for both the founders and the organizers.', 'What were the financial terms of the deal for startups as described in the context, and how did they compare to the deal received by Julian and MIT grad students?', 'How did the concept of funding startups in batches contribute to the sense of community among founders, and what advantages did this approach provide for both the startups and Y Combinator (YC)?', 'What was the original purpose of Hacker News when it was first created, and how did its focus change over time?', \"Describe the impact that Hacker News had on the author's experience with Y Combinator (YC) and how it contributed to their overall stress levels.\", \"In the context of the author's experience with Y Combinator (YC), how did the nature of the work differ from previous jobs, particularly in terms of problem-solving and engagement with startups?\", 'Reflecting on the author\\'s feelings about leadership, what did Kevin Hale mean by the statement \"No one works harder than the boss,\" and how did this perspective influence the author\\'s work ethic at YC?', \"What advice did Robert Morris give regarding the future of Y Combinator, and how did it impact the author's perspective on their work and life trajectory?\", 'Describe the personal challenges the author faced in 2012 and how these challenges influenced their priorities and decisions regarding Y Combinator.', 'What were the reasons that led the author to consider handing over the leadership of Y Combinator (YC) to someone else, and who was ultimately chosen to take over?', \"Describe the impact of the author's mother's health issues on their decision-making regarding YC and their personal life.\", 'What were the reasons behind the decision to reorganize YC and allow Sam to take over as president, and what changes were made to the roles of the original founders?', \"Describe the emotional impact of the author's mother's illness and passing on their work at YC and subsequent decision to pursue painting. How did this shift in focus affect their artistic development throughout 2014?\", 'What motivated the author to shift from working on YC to painting, and what challenges did they face during this transition?', 'Explain the significance of Lisp as described in the text, including its original purpose and how it evolved into a programming language.', 'What was the original purpose of Lisp as conceived by John McCarthy, and how did it differ from its later use as a programming language?', \"Discuss the significance of Steve Russell's contribution to Lisp and how it influenced the language's evolution from a formal model of computation to a programming language.\", 'What challenges did McCarthy face when testing his interpreter, and how did the limitations of computers at the time impact his ability to test more complex interpreters?', 'Describe the process and challenges the author encountered while developing the new Lisp called Bel. How did the decision to refrain from writing essays influence the progress of this project?', 'Describe the challenges the author faced while working on the interpreter \"Bel\" and how these challenges influenced their writing process.', 'What motivated the author to move to England in the summer of 2016, and how did this relocation impact the development of \"Bel\"?', 'What motivated the author and their family to move to England in the summer of 2016, and how did their initial plans for the duration of the stay change over time?', 'In what ways did the author’s writing process evolve after completing the spec for Bel in 2019, and what prompted them to reflect on their past choices regarding what to work on?', \"Discuss the significance of the author's experience with computers, particularly the transition from batch processing to microcomputers, and how this perspective may influence their view on technological evolution.\", \"Analyze the relationship between art and commerce as described in the context, particularly in relation to the RISD students' pursuit of signature styles and the implications of this dynamic in the art world.\", 'In the context of still life painting, what challenges might arise for artists when painting live sitters, as mentioned in the provided text?', 'How did the advancements in commodity processors during the 1990s impact companies like Interleaf, according to the information given?', 'In the context of Viaweb, what was the significance of allowing users to edit Lisp expressions for defining their own page styles, and how did this differ from traditional app editors?', 'According to the text, what lesson does the experience with Y Combinator teach about the impact of customs in industries affected by rapid change, and how does this relate to the evolution of essay writing?', 'Discuss how the evolution of startup costs has influenced the customs and practices of venture capitalists, as mentioned in the context. What implications does this have for independent-minded individuals in rapidly changing fields?', 'Explain the significance of the name \"Y Combinator\" and the choice of its color scheme. How do these choices reflect the organization\\'s identity and its approach to the venture capital landscape in 2005?', \"Discuss the significance of the YC logo's design in relation to its origins. How does it reflect the history of its creator's previous venture, Viaweb?\", 'Analyze the challenges faced by the founder of YC when managing the dual roles of writing essays and running a forum. What implications does this have for public perception and engagement in online discussions?', 'Discuss the implications of misinterpretations in a conversation as described in the text. How does the presence of an individual in the conversation affect the dynamics of responding to these misinterpretations?', \"Analyze the significance of the relationship between personal and professional lives as illustrated by the author's experience of leaving YC. What metaphor is used to describe this separation, and what does it suggest about the depth of their connection?\"]\n"
          ]
        }
      ],
      "source": [
        "# Get the list of queries from the above created dataset\n",
        "\n",
        "queries = list(qa_dataset.queries.values())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bNei9mj4UjN"
      },
      "source": [
        "#### Faithfulness Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mA3NAKevLMC"
      },
      "source": [
        "Let's start with FaithfulnessEvaluator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3ITPhWVrjvP"
      },
      "source": [
        "We will use `gpt-3.5-turbo` for generating response for a given query and `gpt-4` for evaluation.\n",
        "\n",
        "Let's create service_context seperately for `gpt-3.5-turbo` and `gpt-4`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "t-yuVS1iv84q"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ServiceContext is deprecated. Use llama_index.settings.Settings instead, or pass in modules to local functions/methods/interfaces.\nSee the docs for updated usage/migration: \nhttps://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context_migration/",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# gpt-3.5-turbo\u001b[39;00m\n\u001b[1;32m      2\u001b[0m gpt35 \u001b[38;5;241m=\u001b[39m OpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m service_context_gpt35 \u001b[38;5;241m=\u001b[39m \u001b[43mServiceContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_defaults\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpt35\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# gpt-4\u001b[39;00m\n\u001b[1;32m      6\u001b[0m gpt4 \u001b[38;5;241m=\u001b[39m OpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/llama_index/core/service_context.py:31\u001b[0m, in \u001b[0;36mServiceContext.from_defaults\u001b[0;34m(cls, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_defaults\u001b[39m(\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     24\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mServiceContext\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     25\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a ServiceContext from defaults.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    NOTE: Deprecated, use llama_index.settings.Settings instead or pass in\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    modules to local functions/methods/interfaces.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mServiceContext is deprecated. Use llama_index.settings.Settings instead, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor pass in modules to local functions/methods/interfaces.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee the docs for updated usage/migration: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context_migration/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m     )\n",
            "\u001b[0;31mValueError\u001b[0m: ServiceContext is deprecated. Use llama_index.settings.Settings instead, or pass in modules to local functions/methods/interfaces.\nSee the docs for updated usage/migration: \nhttps://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context_migration/"
          ]
        }
      ],
      "source": [
        "# gpt-3.5-turbo\n",
        "gpt35 = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
        "service_context_gpt35 = ServiceContext.from_defaults(llm=gpt35)\n",
        "\n",
        "# gpt-4\n",
        "gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n",
        "service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXdRv7pIt8nw"
      },
      "source": [
        "Create a `QueryEngine` with `gpt-3.5-turbo` service_context to generate response for the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "lrrD5n6w3Oet"
      },
      "outputs": [],
      "source": [
        "# gpt-3.5-turbo\n",
        "gpt35 = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
        "Settings.llm = gpt35\n",
        "\n",
        "vector_index = VectorStoreIndex(nodes)\n",
        "query_engine = vector_index.as_query_engine()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UvtdrdovgZ_"
      },
      "source": [
        "Create a  FaithfulnessEvaluator.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "dbvXvcFnU09s"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
        "gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n",
        "faithfulness_gpt4 = FaithfulnessEvaluator(llm=gpt4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uA1hQ6_F4NoA"
      },
      "source": [
        "Let's evaluate on one question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "9lfyhUuDz6cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Based on the author's experience in graduate school, what realization did they come to regarding the limitations of traditional AI programs, and how did this influence their decision to focus on Lisp?\""
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_query = queries[10]\n",
        "\n",
        "eval_query"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate response first and use faithfull evaluator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "response_vector = query_engine.query(eval_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "MZ6lvmRf3j8i"
      },
      "outputs": [],
      "source": [
        "# Compute faithfulness evaluation\n",
        "\n",
        "eval_result = faithfulness_gpt4.evaluate_response(response=response_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jj79Rq-gn3cv",
        "outputId": "5078aeb7-c620-45d6-dc1f-215e716f4e59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You can check passing parameter in eval_result if it passed the evaluation.\n",
        "eval_result.passing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuVReAjp4eZ_"
      },
      "source": [
        "#### Relevancy Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhomR2dP1Ybf"
      },
      "source": [
        "RelevancyEvaluator is useful to measure if the response and source nodes (retrieved context) match the query. Useful to see if response actually answers the query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13nay3W6tyqj"
      },
      "source": [
        "Instantiate `RelevancyEvaluator` for relevancy evaluation with `gpt-4`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Qw5X_hMB24kC"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.evaluation import RelevancyEvaluator\n",
        "\n",
        "relevancy_gpt4 = RelevancyEvaluator(llm=gpt4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIl_JJhFvNhu"
      },
      "source": [
        "Let's do relevancy evaluation for one of the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Based on the author's experience in graduate school, what realization did they come to regarding the limitations of traditional AI programs, and how did this influence their decision to focus on Lisp?\""
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Pick a query\n",
        "query = queries[10]\n",
        "\n",
        "query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "r9FwcImG3cV0"
      },
      "outputs": [],
      "source": [
        "# Generate response.\n",
        "# response_vector has response and source nodes (retrieved context)\n",
        "response_vector = query_engine.query(query)\n",
        "\n",
        "# Relevancy evaluation\n",
        "eval_result = relevancy_gpt4.evaluate_response(\n",
        "    query=query, response=response_vector\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71j-t0DX3gh4",
        "outputId": "087ca15f-ac6f-449a-8f48-ef257a6d4b0d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You can check passing parameter in eval_result if it passed the evaluation.\n",
        "eval_result.passing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cW5-6T67w_VF",
        "outputId": "5051e37d-f506-4e2f-885a-2ed2ee4cfac7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'YES'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You can get the feedback for the evaluation.\n",
        "eval_result.feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RRdx39SxHxw"
      },
      "source": [
        "#### Batch Evaluator:\n",
        "\n",
        "Now that we have done FaithFulness and Relevancy Evaluation independently. LlamaIndex has `BatchEvalRunner` to compute multiple evaluations in batch wise manner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "-t6Hxrc93jla"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-rYHwE5k5bkMJluP9DMjTmiP8 on tokens per min (TPM): Limit 10000, Used 9559, Requested 1287. Please try again in 5.076s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
            "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-rYHwE5k5bkMJluP9DMjTmiP8 on tokens per min (TPM): Limit 10000, Used 9553, Requested 1500. Please try again in 6.318s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
            "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-rYHwE5k5bkMJluP9DMjTmiP8 on tokens per min (TPM): Limit 10000, Used 9708, Requested 1320. Please try again in 6.168s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
            "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-rYHwE5k5bkMJluP9DMjTmiP8 on tokens per min (TPM): Limit 10000, Used 9616, Requested 1407. Please try again in 6.138s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
            "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-rYHwE5k5bkMJluP9DMjTmiP8 on tokens per min (TPM): Limit 10000, Used 9593, Requested 1431. Please try again in 6.144s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
            "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-rYHwE5k5bkMJluP9DMjTmiP8 on tokens per min (TPM): Limit 10000, Used 9563, Requested 1441. Please try again in 6.024s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
            "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-rYHwE5k5bkMJluP9DMjTmiP8 on tokens per min (TPM): Limit 10000, Used 9905, Requested 1364. Please try again in 7.614s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
            "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.5743040606410246 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-rYHwE5k5bkMJluP9DMjTmiP8 on tokens per min (TPM): Limit 10000, Used 9609, Requested 1500. Please try again in 6.654s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
            "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.7582572032139923 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-rYHwE5k5bkMJluP9DMjTmiP8 on tokens per min (TPM): Limit 10000, Used 9675, Requested 1441. Please try again in 6.696s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
            "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-rYHwE5k5bkMJluP9DMjTmiP8 on tokens per min (TPM): Limit 10000, Used 9623, Requested 1491. Please try again in 6.684s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
            "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.8636899308983665 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-rYHwE5k5bkMJluP9DMjTmiP8 on tokens per min (TPM): Limit 10000, Used 9496, Requested 1407. Please try again in 5.418s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
            "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.322445577025655 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-rYHwE5k5bkMJluP9DMjTmiP8 on tokens per min (TPM): Limit 10000, Used 9463, Requested 1431. Please try again in 5.364s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
            "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.2683640082703143 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-rYHwE5k5bkMJluP9DMjTmiP8 on tokens per min (TPM): Limit 10000, Used 9704, Requested 1491. Please try again in 7.17s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
            "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.0 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-rYHwE5k5bkMJluP9DMjTmiP8 on tokens per min (TPM): Limit 10000, Used 9757, Requested 1500. Please try again in 7.542s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.evaluation import BatchEvalRunner\n",
        "\n",
        "# Let's pick top 10 queries to do evaluation\n",
        "batch_eval_queries = queries[:10]\n",
        "\n",
        "# Initiate BatchEvalRunner to compute FaithFulness and Relevancy Evaluation.\n",
        "runner = BatchEvalRunner(\n",
        "    {\"faithfulness\": faithfulness_gpt4, \"relevancy\": relevancy_gpt4},\n",
        "    workers=1,\n",
        ")\n",
        "\n",
        "# Compute evaluation\n",
        "eval_results = await runner.aevaluate_queries(\n",
        "    query_engine, queries=batch_eval_queries\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAxrc5NF4T1r",
        "outputId": "f80c105c-9d4b-4e10-8707-e4bad2bed9c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's get faithfulness score\n",
        "\n",
        "faithfulness_score = sum(result.passing for result in eval_results['faithfulness']) / len(eval_results['faithfulness'])\n",
        "\n",
        "faithfulness_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGU3_QHW4ajS",
        "outputId": "0e67a5f7-da94-40c4-8aa0-cd8874bb7ae9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's get relevancy score\n",
        "\n",
        "relevancy_score = sum(result.passing for result in eval_results['relevancy']) / len(eval_results['relevancy'])\n",
        "\n",
        "relevancy_score\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Observation:\n",
        "\n",
        "Faithfulness score of `1.0` signifies that the generated answers contain no hallucinations and are entirely based on retrieved context.\n",
        "\n",
        "Relevancy score of `1.0` suggests that the answers generated are consistently aligned with the retrieved context and the queries."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vzjCqAIeRrk1"
      },
      "source": [
        "In this notebook, we have explored how to build and evaluate a RAG pipeline using LlamaIndex, with a specific focus on evaluating the retrieval system and generated responses within the pipeline. \n",
        "\n",
        "LlamaIndex offers a variety of other evaluation modules as well, which you can explore further [here](https://docs.llamaindex.ai/en/stable/module_guides/evaluating/root.html)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
