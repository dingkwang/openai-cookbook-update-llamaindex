{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluate RAG with LlamaIndex"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SxQ2qzb7DPu1"
      },
      "source": [
        "In this notebook we will look into building an RAG pipeline and evaluating it with LlamaIndex. It has following 3 sections.\n",
        "\n",
        "1. Understanding Retrieval Augmented Generation (RAG).\n",
        "2. Building RAG with LlamaIndex.\n",
        "3. Evaluating RAG with LlamaIndex."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jYKmLpi0-AvJ"
      },
      "source": [
        "**Retrieval Augmented Generation (RAG)**\n",
        "\n",
        "LLMs are trained on vast datasets, but these will not include your specific data. Retrieval-Augmented Generation (RAG) addresses this by dynamically incorporating your data during the generation process. This is done not by altering the training data of LLMs, but by allowing the model to access and utilize your data in real-time to provide more tailored and contextually relevant responses.\n",
        "\n",
        "In RAG, your data is loaded and prepared for queries or “indexed”. User queries act on the index, which filters your data down to the most relevant context. This context and your query then go to the LLM along with a prompt, and the LLM provides a response.\n",
        "\n",
        "Even if what you’re building is a chatbot or an agent, you’ll want to know RAG techniques for getting data into your application."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![RAG Overview](../../images/llamaindex_rag_overview.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv9e2I1w-SLF"
      },
      "source": [
        "**Stages within RAG**\n",
        "\n",
        "There are five key stages within RAG, which in turn will be a part of any larger application you build. These are:\n",
        "\n",
        "**Loading:** this refers to getting your data from where it lives – whether it’s text files, PDFs, another website, a database, or an API – into your pipeline. LlamaHub provides hundreds of connectors to choose from.\n",
        "\n",
        "**Indexing:** this means creating a data structure that allows for querying the data. For LLMs this nearly always means creating vector embeddings, numerical representations of the meaning of your data, as well as numerous other metadata strategies to make it easy to accurately find contextually relevant data.\n",
        "\n",
        "**Storing:** Once your data is indexed, you will want to store your index, along with any other metadata, to avoid the need to re-index it.\n",
        "\n",
        "**Querying:** for any given indexing strategy there are many ways you can utilize LLMs and LlamaIndex data structures to query, including sub-queries, multi-step queries and hybrid strategies.\n",
        "\n",
        "**Evaluation:** a critical step in any pipeline is checking how effective it is relative to other strategies, or when you make changes. Evaluation provides objective measures of how accurate, faithful and fast your responses to queries are."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Besynnjg_Cg9"
      },
      "source": [
        "## Build RAG system.\n",
        "\n",
        "Now that we have understood the significance of RAG system, let's build a simple RAG pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install llama-index==0.12.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1NdWoBI_OFR"
      },
      "outputs": [],
      "source": [
        "# The nest_asyncio module enables the nesting of asynchronous functions within an already running async loop.\n",
        "# This is necessary because Jupyter notebooks inherently operate in an asynchronous loop.\n",
        "# By applying nest_asyncio, we can run additional async functions within this existing loop without conflicts.\n",
        "import os\n",
        "import pandas as pd\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from llama_index.core.indices import VectorStoreIndex\n",
        "from llama_index.core.readers import SimpleDirectoryReader\n",
        "from llama_index.core.node_parser import SimpleNodeParser\n",
        "from llama_index.core.evaluation import generate_question_context_pairs\n",
        "from llama_index.core.evaluation.retrieval.evaluator import RetrieverEvaluator\n",
        "from llama_index.llms.openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GocbeIV3rQYc"
      },
      "source": [
        "Set Your OpenAI API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bocDlS3FrP8L"
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY'] = 'YOUR OPENAI API KEY'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oKXIZyFOKIIT"
      },
      "source": [
        "Let's use [Paul Graham Essay text](https://www.paulgraham.com/worked.html) for building RAG pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjyT5I-kKPbl"
      },
      "source": [
        "#### Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUOKqSSeCkEN",
        "outputId": "17c6b9f6-f6f6-4d9f-d75f-a197133f15f1"
      },
      "outputs": [],
      "source": [
        "!mkdir -p 'data/paul_graham/'\n",
        "!curl 'https://raw.githubusercontent.com/run-llama/llama_index/9b28893d0bd26014862d275d3b81a2918b3d05ff/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -o 'data/paul_graham/paul_graham_essay.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7_DxX6xKUbH"
      },
      "source": [
        "#### Load Data and Build Index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vCJln_YKaK-",
        "outputId": "0004d366-35a3-49d4-806c-86a307aaa560"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.settings import Settings\n",
        "from llama_index.embeddings.openai.base import OpenAIEmbedding\n",
        "\n",
        "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
        "# Define an LLM\n",
        "llm = OpenAI(model=\"gpt-4o\")\n",
        "\n",
        "node_parser = SimpleNodeParser(chunk_size=512)\n",
        "nodes = node_parser.get_nodes_from_documents(documents)\n",
        "\n",
        "vector_index = VectorStoreIndex(nodes)\n",
        "print(vector_index._embed_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65FyKpfY_inX"
      },
      "source": [
        "Build a QueryEngine and start querying."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOZBy--R_m3I"
      },
      "outputs": [],
      "source": [
        "query_engine = vector_index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7NVP-N4_rXF"
      },
      "outputs": [],
      "source": [
        "response_vector = query_engine.query(\"What did the author do growing up?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPYx0ycS_x9B"
      },
      "source": [
        "Check response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "BFFLl1d7_4r4",
        "outputId": "74c2bc81-f326-4981-c7ac-92eb836e5e99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The author worked on writing short stories and programming, particularly on an IBM 1401 computer using an early version of Fortran in 9th grade.'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response_vector.response"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IfhTYwoO_8cd"
      },
      "source": [
        "By default it retrieves `two` similar nodes/ chunks. You can modify that in `vector_index.as_query_engine(similarity_top_k=k)`.\n",
        "\n",
        "Let's check the text in each of these retrieved nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcniGJVt_5V8",
        "outputId": "52b9d3da-30e7-460e-ab50-0ac3dc98ea1e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'What I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn\\'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\\n\\nThe language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.\\n\\nI was puzzled by the 1401. I couldn\\'t figure out what to do with it. And in retrospect there\\'s not much I could have done with it. The only form of input to programs was data stored on punched cards, and I didn\\'t have any data stored on punched cards. The only other option was to do things that didn\\'t rely on any input, like calculate approximations of pi, but I didn\\'t know enough math to do anything interesting of that type. So I\\'m not surprised I can\\'t remember any programs I wrote, because they can\\'t have done much. My clearest memory is of the moment I learned it was possible for programs not to terminate, when one of mine didn\\'t. On a machine without time-sharing, this was a social as well as a technical error, as the data center manager\\'s expression made clear.\\n\\nWith microcomputers, everything changed.'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# First retrieved node\n",
        "response_vector.source_nodes[0].get_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"I also worked on spam filters, and did some more painting. I used to have dinners for a group of friends every thursday night, which taught me how to cook for groups. And I bought another building in Cambridge, a former candy factory (and later, twas said, porn studio), to use as an office.\\n\\nOne night in October 2003 there was a big party at my house. It was a clever idea of my friend Maria Daniels, who was one of the thursday diners. Three separate hosts would all invite their friends to one party. So for every guest, two thirds of the other guests would be people they didn't know but would probably like. One of the guests was someone I didn't know but would turn out to like a lot: a woman called Jessica Livingston. A couple days later I asked her out.\\n\\nJessica was in charge of marketing at a Boston investment bank. This bank thought it understood startups, but over the next year, as she met friends of mine from the startup world, she was surprised how different reality was. And how colorful their stories were. So she decided to compile a book of interviews with startup founders.\\n\\nWhen the bank had financial problems and she had to fire half her staff, she started looking for a new job. In early 2005 she interviewed for a marketing job at a Boston VC firm. It took them weeks to make up their minds, and during this time I started telling her about all the things that needed to be fixed about venture capital. They should make a larger number of smaller investments instead of a handful of giant ones, they should be funding younger, more technical founders instead of MBAs, they should let the founders remain as CEO, and so on.\\n\\nOne of my tricks for writing essays had always been to give talks. The prospect of having to stand up in front of a group of people and tell them something that won't waste their time is a great spur to the imagination. When the Harvard Computer Society, the undergrad computer club, asked me to give a talk, I decided I would tell them how to start a startup. Maybe they'd be able to avoid the worst of the mistakes we'd made.\""
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Second retrieved node\n",
        "response_vector.source_nodes[1].get_text()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnvSrTeADNrX"
      },
      "source": [
        "We have built a RAG pipeline and now need to evaluate its performance. We can assess our RAG system/query engine using LlamaIndex's core evaluation modules. Let's examine how to leverage these tools to quantify the quality of our retrieval-augmented generation system."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jAMiOEsT_-o0"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Evaluation should serve as the primary metric for assessing your RAG application. It determines whether the pipeline will produce accurate responses based on the data sources and a range of queries.\n",
        "\n",
        "While it's beneficial to examine individual queries and responses at the start, this approach may become impractical as the volume of edge cases and failures increases. Instead, it may be more effective to establish a suite of summary metrics or automated evaluations. These tools can provide insights into overall system performance and indicate specific areas that may require closer scrutiny.\n",
        "\n",
        "In a RAG system, evaluation focuses on two critical aspects:\n",
        "\n",
        "*   **Retrieval Evaluation:** This assesses the accuracy and relevance of the information retrieved by the system.\n",
        "*   **Response Evaluation:** This measures the quality and appropriateness of the responses generated by the system based on the retrieved information."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "arGZqWQNIitt"
      },
      "source": [
        "#### Question-Context Pair Generation:\n",
        "\n",
        "For the evaluation of a RAG system, it's essential to have queries that can fetch the correct context and subsequently generate an appropriate response. `LlamaIndex` offers a `generate_question_context_pairs` module specifically for crafting questions and context pairs which can be used in the assessment of the RAG system of both Retrieval and Response Evaluation. For more details on Question Generation, please refer to the [documentation](https://docs.llamaindex.ai/en/stable/examples/evaluation/QuestionGeneration.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jK0BWZ88LjDq",
        "outputId": "87d11e23-50cf-4ff3-a449-12264cfd8ab8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 61/61 [01:24<00:00,  1.39s/it]\n"
          ]
        }
      ],
      "source": [
        "qa_dataset = generate_question_context_pairs(\n",
        "    nodes,\n",
        "    llm=llm,\n",
        "    num_questions_per_chunk=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjs2cgTpOfLM"
      },
      "source": [
        "#### Retrieval Evaluation:\n",
        "\n",
        "We are now prepared to conduct our retrieval evaluations. We will execute our `RetrieverEvaluator` using the evaluation dataset we have generated.\n",
        "\n",
        "We first create the `Retriever` and then define two functions: `get_eval_results`, which operates our retriever on the dataset, and `display_results`, which presents the outcomes of the evaluation."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bE1Z77YyPNwE"
      },
      "source": [
        "Let's create the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fV9IdnwLM_aw"
      },
      "outputs": [],
      "source": [
        "retriever = vector_index.as_retriever(similarity_top_k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLSNg2sSPc2U"
      },
      "source": [
        "Define `RetrieverEvaluator`. We use **Hit Rate** and **MRR** metrics to evaluate our Retriever.\n",
        "\n",
        "**Hit Rate:**\n",
        "\n",
        "Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it’s about how often our system gets it right within the top few guesses.\n",
        "\n",
        "**Mean Reciprocal Rank (MRR):**\n",
        "\n",
        "For each query, MRR evaluates the system’s accuracy by looking at the rank of the highest-placed relevant document. Specifically, it’s the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it’s second, the reciprocal rank is 1/2, and so on.\n",
        "\n",
        "Let's check these metrics to check the performance of out retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6V_LCxrPQzp"
      },
      "outputs": [],
      "source": [
        "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
        "    [\"mrr\", \"hit_rate\"], retriever=retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYFgmnpRPX-x"
      },
      "outputs": [],
      "source": [
        "# Evaluate\n",
        "eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3FLlvjoSbI5"
      },
      "source": [
        "Let's define a function to display the Retrieval evaluation results in table format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9T268MhRNxp"
      },
      "outputs": [],
      "source": [
        "def display_results(name, eval_results):\n",
        "    \"\"\"Display results from evaluate.\"\"\"\n",
        "\n",
        "    metric_dicts = []\n",
        "    for eval_result in eval_results:\n",
        "        metric_dict = eval_result.metric_vals_dict\n",
        "        metric_dicts.append(metric_dict)\n",
        "\n",
        "    full_df = pd.DataFrame(metric_dicts)\n",
        "\n",
        "    hit_rate = full_df[\"hit_rate\"].mean()\n",
        "    mrr = full_df[\"mrr\"].mean()\n",
        "\n",
        "    metric_df = pd.DataFrame(\n",
        "        {\"Retriever Name\": [name], \"Hit Rate\": [hit_rate], \"MRR\": [mrr]}\n",
        "    )\n",
        "\n",
        "    return metric_df\n",
        "display_results(\"OpenAI Embedding Retriever\", eval_results)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Observation:\n",
        "\n",
        "The Retriever with OpenAI Embedding  demonstrates a performance with a hit rate of `0.7586`, while the MRR, at `0.6206`, suggests there's room for improvement in ensuring the most relevant results appear at the top. The observation that MRR is less than the hit rate indicates that the top-ranking results aren't always the most relevant. Enhancing MRR could involve the use of rerankers, which refine the order of retrieved documents. For a deeper understanding of how rerankers can optimize retrieval metrics, refer to the detailed discussion in our [blog post](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPxAHE3kSjsT"
      },
      "source": [
        "#### Response Evaluation:\n",
        "\n",
        "1. FaithfulnessEvaluator: Measures if the response from a query engine matches any source nodes which is useful for measuring if the response is hallucinated.\n",
        "2. Relevancy Evaluator: Measures if the response + source nodes match the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zMMJAQvRS8H"
      },
      "outputs": [],
      "source": [
        "# Get the list of queries from the above created dataset\n",
        "queries = list(qa_dataset.queries.values())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bNei9mj4UjN"
      },
      "source": [
        "#### Faithfulness Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mA3NAKevLMC"
      },
      "source": [
        "Let's start with FaithfulnessEvaluator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3ITPhWVrjvP"
      },
      "source": [
        "We will use `gpt-4o-mini` for generating response for a given query and `gpt-4` for evaluation.\n",
        "\n",
        "Let's create service_context seperately for `gpt-4o-mini` and `gpt-4`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXdRv7pIt8nw"
      },
      "source": [
        "Create a `QueryEngine` with `gpt-4o-mini` service_context to generate response for the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrrD5n6w3Oet"
      },
      "outputs": [],
      "source": [
        "# gpt-3.5-turbo\n",
        "gpt4mini = OpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
        "Settings.llm = gpt4mini\n",
        "\n",
        "vector_index = VectorStoreIndex(nodes)\n",
        "query_engine_4mini = vector_index.as_query_engine()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UvtdrdovgZ_"
      },
      "source": [
        "Create a  FaithfulnessEvaluator.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbvXvcFnU09s"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
        "\n",
        "gpt4o = OpenAI(temperature=0, model=\"gpt-4o\")\n",
        "faithfulness_gpt4 = FaithfulnessEvaluator(llm=gpt4o)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uA1hQ6_F4NoA"
      },
      "source": [
        "Let's evaluate on one question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lfyhUuDz6cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'What realization did the author come to during their first year of grad school regarding the limitations of AI programs like SHRDLU, and how did this influence their academic focus?'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_query = queries[10]\n",
        "\n",
        "eval_query"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate response first and use faithfull evaluator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response_vector = query_engine_4mini.query(eval_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZ6lvmRf3j8i"
      },
      "outputs": [],
      "source": [
        "# Compute faithfulness evaluation\n",
        "eval_result = faithfulness_gpt4.evaluate_response(response=response_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jj79Rq-gn3cv",
        "outputId": "5078aeb7-c620-45d6-dc1f-215e716f4e59"
      },
      "outputs": [],
      "source": [
        "# You can check passing parameter in eval_result if it passed the evaluation.\n",
        "eval_result.passing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuVReAjp4eZ_"
      },
      "source": [
        "#### Relevancy Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhomR2dP1Ybf"
      },
      "source": [
        "RelevancyEvaluator is useful to measure if the response and source nodes (retrieved context) match the query. Useful to see if response actually answers the query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13nay3W6tyqj"
      },
      "source": [
        "Instantiate `RelevancyEvaluator` for relevancy evaluation with `gpt-4`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qw5X_hMB24kC"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.evaluation import RelevancyEvaluator\n",
        "\n",
        "relevancy_gpt4 = RelevancyEvaluator(llm=gpt4o)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIl_JJhFvNhu"
      },
      "source": [
        "Let's do relevancy evaluation for one of the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'What realization did the author come to during their first year of grad school regarding the limitations of AI programs like SHRDLU, and how did this influence their academic focus?'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Pick a query\n",
        "query = queries[10]\n",
        "\n",
        "query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9FwcImG3cV0"
      },
      "outputs": [],
      "source": [
        "# Generate response.\n",
        "# response_vector has response and source nodes (retrieved context)\n",
        "response_vector = query_engine_4mini.query(query)\n",
        "\n",
        "# Relevancy evaluation\n",
        "eval_result = relevancy_gpt4.evaluate_response(\n",
        "    query=query, response=response_vector\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71j-t0DX3gh4",
        "outputId": "087ca15f-ac6f-449a-8f48-ef257a6d4b0d"
      },
      "outputs": [],
      "source": [
        "# You can check passing parameter in eval_result if it passed the evaluation.\n",
        "eval_result.passing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cW5-6T67w_VF",
        "outputId": "5051e37d-f506-4e2f-885a-2ed2ee4cfac7"
      },
      "outputs": [],
      "source": [
        "# You can get the feedback for the evaluation.\n",
        "eval_result.feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RRdx39SxHxw"
      },
      "source": [
        "#### Batch Evaluator:\n",
        "\n",
        "Now that we have done FaithFulness and Relevancy Evaluation independently. LlamaIndex has `BatchEvalRunner` to compute multiple evaluations in batch wise manner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-t6Hxrc93jla"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.evaluation import BatchEvalRunner\n",
        "\n",
        "# Let's pick top 10 queries to do evaluation\n",
        "batch_eval_queries = queries[:10]\n",
        "\n",
        "# Initiate BatchEvalRunner to compute FaithFulness and Relevancy Evaluation.\n",
        "runner = BatchEvalRunner(\n",
        "    {\"faithfulness\": faithfulness_gpt4, \"relevancy\": relevancy_gpt4},\n",
        "    workers=1,\n",
        ")\n",
        "\n",
        "# Compute evaluation\n",
        "eval_results = await runner.aevaluate_queries(\n",
        "    query_engine_4mini, queries=batch_eval_queries\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAxrc5NF4T1r",
        "outputId": "f80c105c-9d4b-4e10-8707-e4bad2bed9c0"
      },
      "outputs": [],
      "source": [
        "# Let's get faithfulness score\n",
        "\n",
        "faithfulness_score = sum(result.passing for result in eval_results['faithfulness']) / len(eval_results['faithfulness'])\n",
        "\n",
        "faithfulness_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGU3_QHW4ajS",
        "outputId": "0e67a5f7-da94-40c4-8aa0-cd8874bb7ae9"
      },
      "outputs": [],
      "source": [
        "# Let's get relevancy score\n",
        "\n",
        "relevancy_score = sum(result.passing for result in eval_results['relevancy']) / len(eval_results['relevancy'])\n",
        "\n",
        "relevancy_score\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Observation:\n",
        "\n",
        "Faithfulness score of `1.0` signifies that the generated answers contain no hallucinations and are entirely based on retrieved context.\n",
        "\n",
        "Relevancy score of `1.0` suggests that the answers generated are consistently aligned with the retrieved context and the queries."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vzjCqAIeRrk1"
      },
      "source": [
        "In this notebook, we have explored how to build and evaluate a RAG pipeline using LlamaIndex, with a specific focus on evaluating the retrieval system and generated responses within the pipeline. \n",
        "\n",
        "LlamaIndex offers a variety of other evaluation modules as well, which you can explore further [here](https://docs.llamaindex.ai/en/stable/module_guides/evaluating/root.html)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
